

ZooKeeper，你可把我坑惨了！
==================
> 作者：杜伟，2017.06.08


### 1. 说多了都是泪
前些日子，我们**被自己部署的 ZooKeeper 集群 DDOS 攻击了**。惊不惊喜，意不意外？
肯定有很多朋友会问，怎么会呢？

一般来说确实不可能，**但在一系列条件的配合下，可以把不可能变为可能**，下面就让我给大家一一道来。


### 2. 交代下前提
在讲故事前，有几个前提先跟大家说明下：

#### 前提一
我们**公司服务治理框架使用 Dubbo，注册中心使用 ZooKeeper 集群**。
但是早期规划的时候，为了运维和开发维护简单，将 ZooKeeper 的 IP 放到了 F5 的一个 VS Pool 里，
后续增加或修改节点直接修改 VS Pool，应用中注册中心配置的就是这个 VS 地址。

我们年前已经发现`这种连接方式存在弊端（应用通过 F5 与 ZooKeeper 之间的心跳检测经常中断）`，
并发布了邮件让大家改为**直连 ZooKeeper 集群**(解法)，但没有重视，也没有强制要求。

#### 前提二
我们**有两个数据中心，且关键业务应用都实现了双活**。
但因为一些应用没有部署双活，所以存在`跨机房服务调用与治理`。
因此，早期规划的时候 `ZooKeeper 也是在一个机房部署了集群，
但这种模式在 ZooKeeper 进行消息广播时，会占用较多的专线带宽`。

经监控系统报警后，我们决定在下周进行 **ZooKeeper 部署改造，
一个机房部署普通 ZooKeeper 节点，另一个机房部署 Observer 节点，
同时修改 DNS，让应用连接本机房的 ZooKeeper 节点**(解法)，这样可以**有效降低跨机房带宽占用**。


### 3. 吃一堑
那是一个周五的下午，大家本应该轻松下班，然后享受周末。
突然`监控系统报警 F5 流量过高，导致请求被阻塞。
紧急来到案发现场，得到的信息是 ZooKeeper 集群是流量大户。`
按照**应急处理原则**，我们将所有可能的上线都进行了回滚，但问题没有得到改善。
于是，我们怀疑是不是某些应用连接 ZooKeeper 做了坏事。

通过**网络抓包**，看了下`基本全是 Dubbo 的注册和订阅事件`。
当时我也非常奇怪，`Dubbo 与 ZooKeeper 之间只有在心跳检测失败时才会有大量的广播事件，导致流量上升。`
但我们的 F5 硬件、网络、ZooKeeper 服务都非常正常，一时之间想不到问题的原因，**线索**又中断了。

所以当时的第一决定是启动紧急上线，即按照前提 1 中提到的，
直接修改公司所有应用的配置文件，让大家直连 ZooKeeper 集群，而不通过 F5 做代理。
这一过程很坎坷，因为涉及的应用太多，但是完成改造后，F5 的流量终于降下来了。

正当我们以为终于解决问题时，灾难再次发生，
`跨机房专线流量过高，导致电信机房业务受到影响，还是 ZooKeeper 在疯狂的广播`。
当时的第一想法，是不是同时注册引起的，再加上本身专线带宽就不富裕。
我们提议将前提 2 中的方案也实施，看一下效果。

经过一番折腾后，专线流量也下来了，同时看业务也恢复了正常，加上人困马乏，
我们决定`后续再研究之前抓取的网络包`，先让大家回去休息下，以免明天没人支持。

周六的一天，就这样在偶尔的报警中度过了。
至于**分析网络包**，`看了几个内容后，发现都是正常的服务注册和订阅事件`，也是一头雾水。

然后从周日 5 点开始，`部分业务突然报警，网络 Ping 不通。
经排查发现，受影响的业务都是和 ZooKeeper 部署在同一宿主机的应用，ZooKeeper 吃掉了宿主机 600M 的带宽。`
由于还是没查询出“**内鬼**”是谁，我们只能定下**临时方案：迁移 ZooKeeper，部署到与业务系统无关的宿主机上。**

**[问题根源]()** 正当我们赶赴公司时，突然某个同事`发现一个非常大的网络报文`。
当他把数据包给我看后，真的是柳暗花明的感觉，**“内鬼”终于找到了**。
`原来是一个基础服务，配置了很多路由规则，导致一个包的大小有几百 K，
而我们的机房里有将近两千台机器，ZooKeeper 的性能又是变态的强，于是一切的巧合安排在一起，
一瞬间，流量打死了物理机，打死了专线，打死了 F5......`于是，事情就这样发生了。
当我们**把一堆无用规则删除**后，整个世界安静了，所有报警都消失了。(血的教训)


### 4. 长一智
故事讲到这里似乎就结束了，可是等等，我们在这件事故中到底犯了哪些错误，我们来**复盘**一下：
* 在前提 1、2 中，**我们发现了异常，却*没有刨根问底*，只是凭感觉认为做个简单处理方案即可。
  魔鬼在细节，一切的灾难在发生前都是有征兆的，如果再多一丝细心，多一丝警惕，结果也许会全然不同。**(赞同)
* 同样在前提 1、2 中，整个事件的追踪与跟进完全没有，**_问题没有闭环_**，没有协作推进。
* **_监控不完善_**，不论是`内网的流量监控还是 ZooKeeper 方面的监控都缺失`，完全是靠运气找到了有问题的数据包。
* **_SOA 技术运营缺失_**，由于`功能权限的随意下放，导致造成了灾难的后果。`
* **_技术架构选型与搭建时不够谨慎_**，`不管是 ZooKeeper 集群的搭建还是 Dubbo 在服务治理哪些坑不能踩等等，都没有去仔细讨论与思考。`

**技术没有好坏之分，关键是看人如何运用。一定要避免人云亦云，适合自己公司技术体系的就是最好的。
选好自己公司的技术体系后，最重要的就是做好`运维与监控`工作。**
`监控的每一次报警，就像先兆一样，万万不可掉以轻心。(日常习惯：每天查看监控数据/异常日志，在周报中体现)`
运维的事情，永远是最辛苦最复杂的，但不能因为这些，就将权限随意下放，一定要确保自己可以掌控全局。


[原文](https://www.jianshu.com/p/81601bb6914b)

